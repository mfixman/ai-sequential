data_settings:
    dataset_path: 'tokenized_data_subword_v2'
    vocabulary_path: 'tokenized_data_subword_v2/vocabulary.pickle'
    special_tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']
model_params:
    model_name: 'transformer'
    version: '2'
    encoder_embedding_dim: 512
    decoder_embedding_dim: 512
    hidden_dim: 512
    num_layers: 6
    dropout: 0.2
train:
    epochs: 30
    batch_size: 8
    learning_rate: 0.00001
    loss: 'CrossSimilarity' # 'CrossEntropy' or 'CrossSimilarity' | note: 'CrossSimilarity' works only for transformer
    checkpoint_folder: 'checkpoints'
    load_checkpoint : False
    log: False
infer:
    batch_size: 1
    checkpoint_folder: 'checkpoints'
    load_checkpoint : True
