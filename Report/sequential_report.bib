@inproceedings{Chopra2016AbstractiveSS,
  title={Abstractive Sentence Summarization with Attentive Recurrent Neural Networks},
  author={Sumit Chopra and Michael Auli and Alexander M. Rush},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:133195}
}

@inproceedings{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@misc{huggingface_cnn_dailymail,
  title = {{cnn\_dailymail}},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/datasets/ccdv/cnn_dailymail}}
}

@misc{BERTtokenizer,
  title = {tramsformer.AutoTokenizer},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer}}
}

@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@misc{lstm,
  title = {torch.nn.LSTM},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html}}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{transformer,
  title = {torch.nn.Transformer},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html}}
}

@misc{CrossEnrtopyLoss,
  title = {torch.nn.CrossEntropyLoss},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html}}
}

@misc{cosineSimilarity,
  title = {torch.nn.functional.cosine\_similarity},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html}}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{BERTHugginFace,
  title = {google-bert/bert-base-uncased},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/google-bert/bert-base-uncased}}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{abstCNN,
  title = {Abstractive Text Summarization on CNN / Daily Mail},
  author = {Papers With Code},
  howpublished = {\url{https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily}}
}

@misc{apostrophes,
	title = {Apostrophe --- Effective Writing Practices Tutorial},
	author = {Northern Illinois Univesity},
	howpublished = {\url{https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml}},
	note = {``The apostrophe is not strictly a punctuation mark, but more a part of a word to indicate possessive case, contractions, or omitted letters.''}
}

@misc{BERTNER,
  title = {dslim/bert-base-NER},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/dslim/bert-base-NER}}
}

@misc{cross_entropy_loss,
      title={Cross-Entropy Loss Functions: Theoretical Analysis and Applications}, 
      author={Anqi Mao and Mehryar Mohri and Yutao Zhong},
      year={2023},
      eprint={2304.07288},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bart_model,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pegasus_model,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gliwa-etal-2019-samsum,
    title = "{SAMS}um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
    author = "Gliwa, Bogdan  and
      Mochol, Iwona  and
      Biesek, Maciej  and
      Wawer, Aleksander",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5409",
    doi = "10.18653/v1/D19-5409",
    pages = "70--79"
}

