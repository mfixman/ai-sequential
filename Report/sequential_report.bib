@misc{huggingface_cnn_dailymail,
  title = {{cnn\_dailymail}},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/datasets/ccdv/cnn_dailymail}}
}

@misc{BERTtokenizer,
  title = {tramsformer.AutoTokenizer},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer}}
}

@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@misc{lstm,
  title = {torch.nn.LSTM},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html}}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{transformer,
  title = {torch.nn.Transformer},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html}}
}

@misc{CrossEnrtopyLoss,
  title = {torch.nn.CrossEntropyLoss},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html}}
}

@misc{cosineSimilarity,
  title = {torch.nn.functional.cosine\_similarity},
  author = {PyTorch},
  year = {2022},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html}}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{BERTHugginFace,
  title = {google-bert/bert-base-uncased},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/google-bert/bert-base-uncased}}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{abstCNN,
  title = {Abstractive Text Summarization on CNN / Daily Mail},
  author = {Papers With Code},
  howpublished = {\url{https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily}}
}

@misc{apostrophes,
	title = {Apostrophe --- Effective Writing Practices Tutorial},
	author = {Northern Illinois Univesity},
	howpublished = {\url{https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml}},
	note = {``The apostrophe is not strictly a punctuation mark, but more a part of a word to indicate possessive case, contractions, or omitted letters.''}
}

@misc{BERTNER,
  title = {dslim/bert-base-NER},
  author = {HuggingFace},
  year = {2020},
  howpublished = {\url{https://huggingface.co/dslim/bert-base-NER}}
}

@misc{cross_entropy_loss,
      title={Cross-Entropy Loss Functions: Theoretical Analysis and Applications}, 
      author={Anqi Mao and Mehryar Mohri and Yutao Zhong},
      year={2023},
      eprint={2304.07288},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bart_model,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pegasus_model,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
