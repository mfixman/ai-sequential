\section{Conclusions}
\paragraph{COmment To be changed!}Fine.tuning BERT and GPT models could help improving teh performance by adapting teh mdoels to the abstarctive text sumamrization task - This requires a lot of computational resources and time!!!

\section{Reflections}
\paragraph{Period (".") token}
During our experimentation, we observed that omitting the period (".") token from the model resulted in significant confusion in the source news, as it failed to distinguish between sentences effectively. This issue was evident in the summaries, where unrelated tokens often appeared consecutively.

Consequently, we decided to include the period (".") token in our dataset to better delineate sentences within the sequences. This inclusion aims to improve the clarity and coherence of the generated summaries. However, this approach has a potential drawback, as the period token becomes disproportionately important throughout the dataset, particularly influencing Term Frequency (TF) and Inverse Document Frequency (IDF) metrics.

For future work, we recommend that the TF and IDF metrics should not be calculated for the period token. Alternatively, the end-of-sequence token could be employed to mark sentence boundaries, which might offer a more effective solution.

\paragraph{COMMENT! To be changed}another next step is the integration of Named Entity Recognition (NER) pre-trained model output in the preprocessing step of the dataset. This model would extract information  to include in the embeddings  

