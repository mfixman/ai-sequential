\section{Introduction (Alessandro)}
Abstractive text summarization is a complex and evolving field in natural language processing (NLP) that focuses on generating concise and coherent summaries from longer text documents. Unlike extractive summarization, which involves selecting and concatenating sentences directly from the source text, abstractive summarization aims to produce a summary that captures the essence of the original content in a new, rewritten form. This involves the model understanding the context and generating novel phrases, often utilizing vocabulary not present in the source document.

The task of abstractive summarization poses several challenges. One of the primary difficulties is ensuring that the generated summary is both coherent and relevant to the main points of the source text. Additionally, the model must handle issues such as maintaining grammatical correctness and avoiding redundancy, which are crucial for producing high-quality summaries.

Recent advancements in deep learning have significantly improved the performance of abstractive summarization models. Sequence-to-sequence (Seq2Seq) models, particularly those incorporating attention mechanisms, have become the standard approach for this task. The attentional encoder-decoder model \cite{bahdanau2014neural} introduced the concept of allowing the model to focus on different parts of the input sequence while generating each word of the output sequence.

Following this, several improvements and variations have been developed such as a model where the decoder was replaced with an RNN\cite{nallapati2016abstractive}. 

In addition to RNN-based models, transformer-based architectures have also been explored extensively. Transformer model \cite{transformer} relies on self-attention mechanisms to process the entire input sequence simultaneously, offering better parallelization and efficiency during training. Subsequent models like BERT \cite{devlin2018bert} and GPT-2 \cite{radford2019language} have been adapted for summarization tasks, leveraging their pre-training on large corpora to improve performance in low-data settings.

The primary objective of this research is to develop and evaluate different neural network architectures for abstractive text summarization, focusing on enhancing model performance through various techniques. Our methodology includes the implementation of Seq2Seq models with attention mechanisms, transformer models, and hybrid approaches that combine pre-trained models like BERT and GPT-2 with custom encoder-decoder architectures. We employ the CNN/Daily Mail dataset\cite{see-etal-2017-get,gliwa-etal-2019-samsum}, a well-known benchmark in the field, to train and test our models. The dataset contains news articles and their corresponding highlights, which serve as the ground truth summaries. We utilized the 3.0.0 version of the dataset from the \textit{Hugging Face} datasets library \cite{huggingface_cnn_dailymail}, which contains 287,113 training pairs, 13,368 validation pairs and 11,490 test pairs. The source documents in the training set have 781 words on an average while the summaries consist of 56 words.

