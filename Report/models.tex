\section{Models}
\subsection{Baseline Model: Seq2Seq}

We initially implemented a Seq2Seq model\cite{sutskever2014sequence} using an encoder-decoder architecture with Long Short-Term Memory (LSTM) layers\cite{lstm}.

The Seq2Seq model processes an input sequence and generates an output sequence, predicting one token at a time in an autoregressive manner. The general functioning of the Seq2Seq model can be described as follows:

\paragraph{Encoder:}
The encoder takes an input sequence \( X = (x_1, x_2, \ldots, x_n) \) and processes it through multiple LSTM layers to produce a set of hidden states \( \mathbf{H} = (\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n) \). Before the LSTM layers, an embedding layer is used to convert the input tokens into dense vectors of a fixed size. The final hidden state and cell state of the encoder are passed to all the layers of the decoder as the initial states. The encoder's operations can be expressed as:
\begin{equation}
    \mathbf{e}_t = \text{Embedding}(x_t) \,\, \rightarrow \,\, \mathbf{h}_t = \text{LSTM}(\mathbf{e}_t, \mathbf{h}_{t-1})
\end{equation}
where \( \mathbf{e}_t \) is the embedding of the input token \( x_t \) and \( \mathbf{h}_t \) is the hidden state at time step \( t \).

\paragraph{Decoder:}
The decoder generates the output sequence \( Y = (y_1, y_2, \ldots, y_m) \) one token at a time. At each time step \( t \), the decoder takes the previous token \( y_{t-1} \), the previous hidden state, and the previous cell state as inputs, and produces the current token \( y_t \). An embedding layer is also used in the decoder to convert the input tokens into dense vectors before passing them to the LSTM layers. Initially, the decoder receives the start-of-sequence token \( \langle \text{sos} \rangle \) as the first input. The decoder's operations can be described as:
\begin{equation}
    \mathbf{e}_t' = \text{Embedding}(y_{t-1}) \,\, \rightarrow \,\, \mathbf{h}_t = \text{LSTM}(\mathbf{e}_t', \mathbf{h}_{t-1})
\end{equation}
where \( \mathbf{e}_t' \) is the embedding of the previous output token \( y_{t-1} \), \( \mathbf{h}_t \) is the hidden state at time step \( t \).

\subsubsection{Bahdanau Attention}

As an improvement to the basic Seq2Seq model, we incorporated Bahdanau Attention\cite{bahdanau2014neural} only in the decoder. This attention mechanism allows the model to focus on relevant parts of the input sequence at each decoding step, improving the quality of the generated summaries.

The Bahdanau attention weights can be formulated as:
\begin{equation}
    \alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})}
\end{equation}
where \( e_{t,i}\) are the attention scores based on the alignment between the decoder's hidden state and the encoder's output states:
\begin{equation}
    e_{t,i} = \mathbf{v}^T \tanh(\mathbf{W}_a \mathbf{h}_t + \mathbf{U}_a \mathbf{h}_i)
\end{equation}
where \( \mathbf{h}_t \) is the decoder's hidden state at time step \( t \), \( \mathbf{H} = \{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_T\} \) is the encoder's outputs, \( \mathbf{W}_a \) and \( \mathbf{U}_a \) are learnable weight matrices, and \( \mathbf{v} \) is a learnable parameter vector.
The output of the attention layer is:
\begin{equation}
    \mathbf{h}'_t = \tanh(\mathbf{W}_c [\mathbf{c}_t ; \mathbf{h}_t])
\end{equation}
where \( \mathbf{W}_c \) is a learnable weight matrix and \( [\mathbf{c}_t ; \mathbf{h}_t] \) denotes the concatenation of the decoder's hidden state and the context vector:
\begin{equation}
    \mathbf{c}_t = \sum_{i=1}^{T} \alpha_{t,i} \mathbf{h}_i
\end{equation}

The decoder uses this context vector \( \mathbf{c}_t \) along with its hidden states to generate the output tokens. This mechanism enhances the model's ability to focus on the most relevant parts of the input sequence during each step of the decoding process.

The Seq2Seq model with Bahdanau attention was trained using top-10 sampling and teacher forcing with an initial ratio of 0.5, which decreased by 0.0005 at every step. The teacher forcing ratio allows the model to gradually learn to generate sequences independently by reducing the dependency on the correct previous token over time.

\subsection{Transformer Model}

The Transformer model\cite{vaswani2017attention}, implemented using PyTorch's \textit{nn.Transformer} class\cite{transformer}, is another architecture usually employed for abstractive text summarization. Unlike the Seq2Seq model, the Transformer relies on self-attention mechanisms to process the entire input sequence simultaneously, allowing for more parallelization and efficient training.

\subsubsection{Self-Attention Mechanism}

The self-attention mechanism is a key component of the Transformer model, allowing the model to weigh the importance of different tokens in the input sequence dynamically. The attention mechanism operates as follows:

Given an input sequence \(\mathbf{X} \in \mathbb{R}^{n \times d}\), where \(n\) is the sequence length and \(d\) is the embedding dimension, we compute the Query (\(\mathbf{Q}\)), Key (\(\mathbf{K}\)), and Value (\(\mathbf{V}\)) matrices as:
\begin{equation}
     \mathbf{Q} = \mathbf{X}\mathbf{W^Q}, \quad \mathbf{K} = \mathbf{XW^K}, \quad \mathbf{V} = \mathbf{XW^V}
\end{equation}
where \(\mathbf{W^Q}, \mathbf{W^K}, \mathbf{W^V} \in \mathbb{R}^{d \times d_k}\) are learnable weight matrices.

The attention scores are computed as:
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}
where \(d_k\) is the dimensionality of the queries and keys.

Multi-head attention extends this by allowing the model to jointly attend to information from different representation subspaces at different positions. This is achieved by concatenating the outputs of \(h\) attention heads:
\begin{equation}
    \text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W^O}
\end{equation}
where \(\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W^Q}_i, \mathbf{K}\mathbf{W^K}_i, \mathbf{V}\mathbf{W^V}_i)\), and \(\mathbf{W^O} \in \mathbb{R}^{hd_k \times d}\).

\subsubsection{Positional Encoding and Masking}

Since the Transformer model lacks recurrence, it does not have a built-in sense of the order of words in a sequence. To address this, positional encodings are added to the input embeddings to provide information about the position of each token within the sequence.

The positional encoding for a position \(pos\) and dimension \(i\) is defined as:
\begin{equation}
	\begin{aligned}
		PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
		PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d}}\right)
	\end{aligned}
\end{equation}
where \(d\) is the dimensionality of the embeddings. These positional encodings are added to the input embeddings to inject positional information.

Masking is also crucial in the Transformer architecture. We employed two types of masks: a padding mask to prevent the model from attending to padding tokens in the input and a target sequence mask to ensure that the model only attends to previous tokens in the target sequence during training.


\subsection{BERTformer}
Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert} is a language model designed to understand the context of words in a sentence by considering both the left and right surroundings (bidirectional context). Unlike traditional left-to-right or right-to-left training, BERT is trained on masked language modeling (MLM) and next sentence prediction (NSP) tasks, enabling it to capture a deep understanding of language nuances and context.

In our approach, we implemented a custom transformer model, named BERTformer, by combining the pre-trained frozen BERT model as the encoder and the standard Transformer decoder from the PyTorch \textit{nn.Transformer} library. This hybrid architecture aims to leverage BERT's robust embeddings effectively capture the meaning and context of news articles, thus facilitating better summarization.

The encoder component of BERTformer utilizes the BERT-base model from HuggingFace \cite{BERTHugginFace}, which consists of 12 transformer layers with 768 hidden units and 12 self-attention heads. The BERT encoder had been fed with the embedded masked input sequences, including positional embedding, TF and IDF metrics. The input sequence had to be truncated to a maximum size of 512 tokens. The output of the BERT encoder, which consists of contextualized embeddings, is passed to the transformer decoder, which is the decoder class of the Pytorch's implementation.

