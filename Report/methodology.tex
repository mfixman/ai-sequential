\section{Methodology}

\subsection{Data preprocessing}
\label{data_preprocessing}

\subsubsection{Tokenisation}
For processing the data must be separated into tokens, which are later encoded into the models (which include the embeddings).

This tokenisation is not trivial, since having too many words can unnecessarily produce high dimensionality in the embeddings which can make the model slower and less efficient.
To prevent this we use a pretrained BERT subword tokeniser\cite{BERTtokenizer}.

\begin{figure}[h]
	\newcommand{\hh}{\texttt{\#\#}}
	\newcommand{\tokbox}[1]{\fbox{\strut\centering #1}}
	\centering
	\fbox{\parbox{\textwidth}{\fbox{\parbox{\textwidth - 13pt}{\small\sffamily the head of pakistan's ruling coalition announced thursday that the government will \\ move to impeach president pervez musharraf}}}} \\
	\fbox{\parbox[m][][t]{\textwidth}{\small \sffamily
		 \tokbox{the}%
		 \tokbox{head}%
		 \tokbox{of}%
		 \tokbox{pakistan}%
		 \tokbox{'}%
		 \tokbox{s}%
		 \tokbox{ruling}%
		 \tokbox{coalition}%
		 \tokbox{announced}%
		 \tokbox{thursday}%
		 \tokbox{that}%
		 \tokbox{the}%
		 \tokbox{government}%
		 \tokbox{will}
		 \tokbox{move}%
		 \tokbox{to}%
		 \tokbox{imp}%
		 \tokbox{\hh{}ea}%
		 \tokbox{\hh{}ch}%
		 \tokbox{president}%
		 \tokbox{per}%
		 \tokbox{\hh{}vez}%
		 \tokbox{mu}%
		 \tokbox{\hh{}sha}%
		 \tokbox{\hh{}rra}%
		 \tokbox{\hh{}f}
	 }}
	\caption{Example of BERT tokeniser. Long words not in the dictionary, such as \textsf{``musharraf''}, get converted into several tokens that can be shared between words.}
\end{figure}

Each one of these tokens (including special tokens such as \texttt{<SOS>}) is transformer to a single integer from 0 to \num{30522}.
The embedding layer, present in all our models, returns a spatial embedding.

\subsubsection{Punctuation}
Most punctuation is removed from the dataset, as it doesn't help in our objective of summarising a text.
Periods \textsf{`\textbf{.}'} are retained, since they help separate between the many news in our input text.

\subsubsection{TF and IDF Metrics Embedding}

All of our enhanced models compute Term Frequency (TF) and Inverse Document Frequency (IDF) metrics for the dataset\cite{nallapati2016abstractive}.
These metrics measure how frequently a term occurs in a document, and how important this term is respectively.
\begin{align}
	\text{TF}(t) &= \frac{\text{Number of times term $t$ appears in a document}}{\text{Total number of terms in the document}} \\[1ex]
	\text{IDF}(t) &= \log \left( \frac{\text{Total number of documents}}{\text{Number of documents with term $t$ in it}} \right)
\end{align}

The inclusion of these embeddings aimed to provide the model with additional information about the importance of words in the context of the entire dataset, potentially improving the quality of the generated summaries.

\subsection{Loss Function(s)}

All loss functions are added to al $L_1$ regularisation loss with $\lambda = 10^{-5}$.

\subsubsection{Categorical Cross-Entropy Loss}

Part of our loss function is the Categorical Cross-Entropy Loss\cite{cross_entropy_loss}, which achieves a smooth gradient function by calculating the loss with the logits of the result rather than the result itself.
\begin{equation}
	\mathcal{L}_\text{CCE} = - \frac{1}{N} \frac{1}{W} \sum_{n=1}^{N} \sum^W_{w = 1} \log(p_{n,w,y_{n,w}})
\end{equation}

This loss is calculated separately between each pair of words between the prediction and the target set, where it's averaged between all words and all batches.
While it does predict how close the prediction is to exact value of the target, it does not separate between a slightly different and a completely unrelated word.

\subsubsection{Cosine Similarity Loss}
\label{cosine_similarity_loss}

We can use the spatial information from the embeddings to estimate how close each predicted word to the real target\cite{cosineSimilarity}.
\begin{equation}
	\mathcal{L}_\text{CS} = 1 - \frac{1}{N} \sum_{n=1}^{N} \sum^W_{w = 1} \frac{\mathbf{x}_{n,w} \cdot \mathbf{y}_{n,w}}{\|\mathbf{x}_{n,w}\| \|\mathbf{y}_{n, w}\|}	
\end{equation}

This loss function would help us reach a result that's meaningful rather than identical to the target, as in the CCE loss.
Additionally, we do not average its value by words to attempt to predict a shorter sequence.

However, when training embeddings as part of a larger model, relying solely on cosine similarity loss can be problematic since the loss is minimized when the embeddings of all words become very close to each other.
This reduces the ability of the embeddings to capture distinct and meaningful differences between words, which might collapse of embeddings into a small region of the space can lead to poor generalization and a lack of discrimination between different words.

\subsection{Combined Cross Similarity}
We introduce \emph{Combined Cross Similarity Loss}, which combines both CCE and CS losses into a single category in a way that's controlled by a parameter $\varkappa$.
\begin{equation}
    \mathcal{L} = (1 - \varkappa) \mathcal{L}_{\text{CCE}} + \varkappa \mathcal{L}_{\text{CS}}
\end{equation}

A higher $\varkappa$ gives more weight to the cosine-similarity loss component of the loss, while a lower one gives more weight to the categorical cross-entropy component.
The ideal value, along with other components, will be found in a parameter sweep.

Unfortunately, the section in \cref{param_sweep_section} showed that having $\varkappa = 0$ produces a better result for low learning rate.
This probably speaks of the strength of categorical cross-entropy as a loss.

\subsection{Scoring Functions}
\label{scoring_section}
\subsubsection{ROUGE-1 and ROUGE-2}
To evaluate the performance of our models, we utilized the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics, which are the standard benchmark for abstractive text summarization tasks\cite{abstCNN}.

Specifically, we calculated the ROUGE-1 and ROUGE-2 scores, which measure the overlap of unigrams and bigrams between the generated summaries and the reference summaries, respectively. These scores are calculated based on precision, recall, and F1-score:
\begin{equation}
	\begin{aligned}
		\text{Prec}^{\text{ROUGE-N}} &= \frac{\left|\text{N-grams}_{\text{Prediction}} \cap \text{N-grams}_{\text{Target}}\right|}{\left|\text{N-grams}_{\text{Prediction}}\right|} \\
		\text{Rec}^{\text{ROUGE-N}} &= \frac{\left|\text{N-grams}_{\text{Prediction}} \cap \text{N-grams}_{\text{Target}}\right|}{\left|\text{N-grams}_{\text{Target}}\right|} \\[1em]
		\text{ROUGE-N} = F_1^{\text{ROUGE-N}} &= \frac{2 \cdot \text{Prec}^{\text{ROUGE-N}} \cdot \text{Rec}^{\text{ROUGE-N}}}{\text{Prec}^{\text{ROUGE-N}} + \text{Rec}^{\text{ROUGE-N}}}
	\end{aligned}
\end{equation}

In contrast to typical ROUGE implementations, which take text as input and tokenize it internally, we developed a custom approach to calculate the ROUGE metrics directly on the token indices.
This method ensures that the evaluation is consistent with the tokenization scheme used during model training and inference, and that the results can be found quickly using GPU processing.

\subsubsection{Cosine Similarity Score}
\label{cosine_similarity_score}
In addition to the cosine similarity loss used in \cref{cosine_similarity_loss}, we use the inverse of the cosine similarity loss as a scoring and comparing models.

\subsection{Early Stopping}
\label{early_stopping}
We aggressively use early stopping to prevent overfitting and training for longer than necessary.
To achieve this we only choose the model generated by each one of our models in the epoch with minimal validation loss.

In plots such as \cref{baseline_rouges}, the parts after the early stopping are shown slightly transparent; the final model is the one trained at the end of the non-transparent line.
