\section{Methodology}

\subsection{Data preprocessing}

\subsubsection{Punctuation and Stop Words}

Most punctuation is removed from the dataset, as it doesn't help in our objective of summarising a text.
Periods \textsf{`\textbf{.}'} are retained, since they help separate between the many news in our input text.
We also do not delete apostrophes \textsf{`\textbf{'}'}, as they are an integral part of the English language\cite{apostrophes}.

\subsubsection{Tokenisation}

For processing the data must be separated into tokens, which are later one-hot-encoded into the models (which include the embeddings).

This tokenisation is not trivial, since having too many words can unnecessarily produce high dimensionality in the embeddings which can make the model slower and more efficient.
To prevent this we use the existing BERT subword tokeniser\cite{BERTtokenizer}.

\begin{figure}[h]
	\newcommand{\hh}{\texttt{\#\#}}
	\newcommand{\tokbox}[1]{\fbox{\strut\centering #1}}
	\fbox{\parbox{\textwidth}{\small\sffamily the head of pakistan's ruling coalition announced thursday that the government will \\ move to impeach president pervez musharraf}} \\
	\parbox[m][][t]{\textwidth}{\small \sffamily
		 \tokbox{the}%
		 \tokbox{head}%
		 \tokbox{of}%
		 \tokbox{pakistan}%
		 \tokbox{'}%
		 \tokbox{s}%
		 \tokbox{ruling}%
		 \tokbox{coalition}%
		 \tokbox{announced}%
		 \tokbox{thursday}%
		 \tokbox{that}%
		 \tokbox{the}%
		 \tokbox{government}%
		 \tokbox{will}
		 \tokbox{move}%
		 \tokbox{to}%
		 \tokbox{imp}%
		 \tokbox{\hh{}ea}%
		 \tokbox{\hh{}ch}%
		 \tokbox{president}%
		 \tokbox{per}%
		 \tokbox{\hh{}vez}%
		 \tokbox{mu}%
		 \tokbox{\hh{}sha}%
		 \tokbox{\hh{}rra}%
		 \tokbox{\hh{}f}
	 }
	\caption{Example of BERT tokeniser. Long words not in the dictionary, such as \textsf{``musharraf''}, get converted into several tokens that can be shared between words.}
\end{figure}

Each one of these tokens (including special tokens such as \texttt{<SOS>}) is transformer to a single integer from 0 to \num{30522}.
The embedding layer, present in all our models, takes a one-hot-encoded vector with this number and returns a spacial embedding.

\subsubsection{TF and IDF Metrics Embedding}

All of our enhanced models compute Term Frequency (TF) and Inverse Document Frequency (IDF) metrics for the dataset\cite{nallapati2016abstractive}.
These metrics measure how frequently a term occurs in a document, and how important this term is respectively.
\begin{align}
	\text{TF}(t) &= \frac{\text{Number of times term $t$ appears in a document}}{\text{Total number of terms in the document}} \\[1ex]
	\text{IDF}(t) &= \log \left( \frac{\text{Total number of documents}}{\text{Number of documents with term $t$ in it}} \right)
\end{align}

The inclusion of these embeddings aimed to provide the model with additional information about the importance of words in the context of the entire dataset, potentially improving the quality of the generated summaries.
This is in addition to \emph{positional encodings}, which are added later to the embeddings in transformer models.

\subsection{Seq2Seq Model}

We initially implemented a Seq2Seq model\cite{sutskever2014sequence} using an encoder-decoder architecture with Long Short-Term Memory (LSTM) layers\cite{lstm}.

The Seq2Seq model processes an input sequence and generates an output sequence, predicting one token at a time in an autoregressive manner. The general functioning of the Seq2Seq model can be described as follows:

\paragraph{Encoder:}
The encoder takes an input sequence \( X = (x_1, x_2, \ldots, x_n) \) and processes it through multiple LSTM layers to produce a set of hidden states \( \mathbf{H} = (\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n) \). Before the LSTM layers, an embedding layer is used to convert the input tokens into dense vectors of a fixed size. The final hidden state and cell state of the encoder are passed to all the layers of the decoder as the initial states. The encoder's operations can be expressed as:
\begin{equation}
    \mathbf{e}_t = \text{Embedding}(x_t) \,\, \rightarrow \,\, \mathbf{h}_t = \text{LSTM}(\mathbf{e}_t, \mathbf{h}_{t-1})
\end{equation}
where \( \mathbf{e}_t \) is the embedding of the input token \( x_t \) and \( \mathbf{h}_t \) is the hidden state at time step \( t \).

\paragraph{Decoder:}
The decoder generates the output sequence \( Y = (y_1, y_2, \ldots, y_m) \) one token at a time. At each time step \( t \), the decoder takes the previous token \( y_{t-1} \), the previous hidden state, and the previous cell state as inputs, and produces the current token \( y_t \). An embedding layer is also used in the decoder to convert the input tokens into dense vectors before passing them to the LSTM layers. Initially, the decoder receives the start-of-sequence token \( \langle \text{sos} \rangle \) as the first input. The decoder's operations can be described as:
\begin{equation}
    \mathbf{e}_t' = \text{Embedding}(y_{t-1}) \,\, \rightarrow \,\, \mathbf{h}_t = \text{LSTM}(\mathbf{e}_t', \mathbf{h}_{t-1})
\end{equation}
where \( \mathbf{e}_t' \) is the embedding of the previous output token \( y_{t-1} \), \( \mathbf{h}_t \) is the hidden state at time step \( t \).

\subsubsection{Bahdanau Attention}

As an improvement to the basic Seq2Seq model, we incorporated Bahdanau Attention\cite{bahdanau2014neural} only in the decoder. This attention mechanism allows the model to focus on relevant parts of the input sequence at each decoding step, improving the quality of the generated summaries.

The Bahdanau attention weights can be formulated as:
\begin{equation}
    \alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})}
\end{equation}
where \( e_{t,i}\) are the attention scores based on the alignment between the decoder's hidden state and the encoder's output states:
\begin{equation}
    e_{t,i} = \mathbf{v}^T \tanh(\mathbf{W}_a \mathbf{h}_t + \mathbf{U}_a \mathbf{h}_i)
\end{equation}
where \( \mathbf{h}_t \) is the decoder's hidden state at time step \( t \), \( \mathbf{H} = \{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_T\} \) is the encoder's outputs, \( \mathbf{W}_a \) and \( \mathbf{U}_a \) are learnable weight matrices, and \( \mathbf{v} \) is a learnable parameter vector.
The output of the attention layer is:
\begin{equation}
    \mathbf{h}'_t = \tanh(\mathbf{W}_c [\mathbf{c}_t ; \mathbf{h}_t])
\end{equation}
where \( \mathbf{W}_c \) is a learnable weight matrix and \( [\mathbf{c}_t ; \mathbf{h}_t] \) denotes the concatenation of the decoder's hidden state and the context vector:
\begin{equation}
    \mathbf{c}_t = \sum_{i=1}^{T} \alpha_{t,i} \mathbf{h}_i
\end{equation}

The decoder uses this context vector \( \mathbf{c}_t \) along with its hidden states to generate the output tokens. This mechanism enhances the model's ability to focus on the most relevant parts of the input sequence during each step of the decoding process.

\paragraph{}
The Seq2Seq model with Bahdanau attention was trained using top-10 sampling and teacher forcing with an initial ratio of 0.5, which decreased by 0.0005 at every step. The teacher forcing ratio allows the model to gradually learn to generate sequences independently by reducing the dependency on the correct previous token over time.

\subsection{Transformer Model}

The Transformer model\cite{vaswani2017attention}, implemented using PyTorch's \textit{nn.Transformer} class\cite{transformer}, is another architecture usually employed for abstractive text summarization. Unlike the Seq2Seq model, the Transformer relies on self-attention mechanisms to process the entire input sequence simultaneously, allowing for more parallelization and efficient training.

\subsubsection{Self-Attention Mechanism}

The self-attention mechanism is a key component of the Transformer model, allowing the model to weigh the importance of different tokens in the input sequence dynamically. The attention mechanism operates as follows:

Given an input sequence \(\mathbf{X} \in \mathbb{R}^{n \times d}\), where \(n\) is the sequence length and \(d\) is the embedding dimension, we compute the Query (\(\mathbf{Q}\)), Key (\(\mathbf{K}\)), and Value (\(\mathbf{V}\)) matrices as:
\begin{equation}
     \mathbf{Q} = \mathbf{X}\mathbf{W^Q}, \quad \mathbf{K} = \mathbf{XW^K}, \quad \mathbf{V} = \mathbf{XW^V}
\end{equation}
where \(\mathbf{W^Q}, \mathbf{W^K}, \mathbf{W^V} \in \mathbb{R}^{d \times d_k}\) are learnable weight matrices.

The attention scores are computed as:
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}
where \(d_k\) is the dimensionality of the queries and keys.

Multi-head attention extends this by allowing the model to jointly attend to information from different representation subspaces at different positions. This is achieved by concatenating the outputs of \(h\) attention heads:
\begin{equation}
    \text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W^O}
\end{equation}
where \(\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W^Q}_i, \mathbf{K}\mathbf{W^K}_i, \mathbf{V}\mathbf{W^V}_i)\), and \(\mathbf{W^O} \in \mathbb{R}^{hd_k \times d}\).

\subsubsection{Positional Encoding and Masking}

Since the Transformer model lacks recurrence, it does not have a built-in sense of the order of words in a sequence. To address this, positional encodings are added to the input embeddings to provide information about the position of each token within the sequence.

The positional encoding for a position \(pos\) and dimension \(i\) is defined as:
\begin{equation}
	\begin{aligned}
		PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
		PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d}}\right)
	\end{aligned}
\end{equation}
where \(d\) is the dimensionality of the embeddings. These positional encodings are added to the input embeddings to inject positional information.

Masking is also crucial in the Transformer architecture. We employed two types of masks: a padding mask to prevent the model from attending to padding tokens in the input and a target sequence mask to ensure that the model only attends to previous tokens in the target sequence during training.


\subsection{Cross Similarity Loss Function}
The primary objective of our abstractive text summarization task is to generate summaries that are not only syntactically correct but also semantically meaningful. Traditional cross-entropy loss, which is commonly used for sequence-to-sequence models, primarily focuses on token-level accuracy by penalizing the incorrect predictions at each step. However, this approach does not account for the semantic similarity between the generated summary and the reference summary.

To address this limitation, we propose a custom loss function, \textit{CrossSimilarityLoss}, which combines the traditional cross-entropy loss\cite{CrossEnrtopyLoss} with a semantic similarity loss, using a $\varkappa$ hyperparameter weight:

\begin{equation}
    \mathcal{L} = (1 - \varkappa) \mathcal{L}_{\text{CE}} + \varkappa \mathcal{L}_{\text{semantic}}
\end{equation}

The semantic similarity loss is computed using cosine similarity\cite{cosineSimilarity} between the representation of the predicted and target sequences. These representation are collected from the output of the decoder of the transformer model. The cosine similarity is then averaged over the sequence and the batches.

By incorporating semantic similarity, we aim to generate summaries that better capture the overall meaning and context of the source text, beyond mere token accuracy.

\subsection{Metrics}

\subsection{Reflections}
\paragraph{Period (".") token}
During our experimentation, we observed that omitting the period (".") token from the model resulted in significant confusion in the source news, as it failed to distinguish between sentences effectively. This issue was evident in the summaries, where unrelated tokens often appeared consecutively.

Consequently, we decided to include the period (".") token in our dataset to better delineate sentences within the sequences. This inclusion aims to improve the clarity and coherence of the generated summaries. However, this approach has a potential drawback, as the period token becomes disproportionately important throughout the dataset, particularly influencing Term Frequency (TF) and Inverse Document Frequency (IDF) metrics.

For future work, we recommend that the TF and IDF metrics should not be calculated for the period token. Alternatively, the end-of-sequence token could be employed to mark sentence boundaries, which might offer a more effective solution.
