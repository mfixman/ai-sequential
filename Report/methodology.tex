\section{Methodology}

\subsection{Data preprocessing}

\subsubsection{Punctuation and Stop Words}
Most punctuation is removed from the dataset, as it doesn't help in our objective of summarising a text.
Periods \textsf{`\textbf{.}'} are retained, since they help separate between the many news in our input text.
We also do not delete apostrophes \textsf{`\textbf{'}'}, as they are an integral part of the English language\cite{apostrophes}.

\subsubsection{Tokenisation}
For processing the data must be separated into tokens, which are later one-hot-encoded into the models (which include the embeddings).

This tokenisation is not trivial, since having too many words can unnecessarily produce high dimensionality in the embeddings which can make the model slower and more efficient.
To prevent this we use the existing BERT subword tokeniser\cite{BERTtokenizer}.

\begin{figure}[h]
	\newcommand{\hh}{\texttt{\#\#}}
	\newcommand{\tokbox}[1]{\fbox{\strut\centering #1}}
	\centering
	\fbox{\parbox{\textwidth}{\fbox{\parbox{\textwidth - 13pt}{\small\sffamily the head of pakistan's ruling coalition announced thursday that the government will \\ move to impeach president pervez musharraf}}}} \\
	\fbox{\parbox[m][][t]{\textwidth}{\small \sffamily
		 \tokbox{the}%
		 \tokbox{head}%
		 \tokbox{of}%
		 \tokbox{pakistan}%
		 \tokbox{'}%
		 \tokbox{s}%
		 \tokbox{ruling}%
		 \tokbox{coalition}%
		 \tokbox{announced}%
		 \tokbox{thursday}%
		 \tokbox{that}%
		 \tokbox{the}%
		 \tokbox{government}%
		 \tokbox{will}
		 \tokbox{move}%
		 \tokbox{to}%
		 \tokbox{imp}%
		 \tokbox{\hh{}ea}%
		 \tokbox{\hh{}ch}%
		 \tokbox{president}%
		 \tokbox{per}%
		 \tokbox{\hh{}vez}%
		 \tokbox{mu}%
		 \tokbox{\hh{}sha}%
		 \tokbox{\hh{}rra}%
		 \tokbox{\hh{}f}
	 }}
	\caption{Example of BERT tokeniser. Long words not in the dictionary, such as \textsf{``musharraf''}, get converted into several tokens that can be shared between words.}
\end{figure}

Each one of these tokens (including special tokens such as \texttt{<SOS>}) is transformer to a single integer from 0 to \num{30522}.
The embedding layer, present in all our models, takes a one-hot-encoded vector with this number and returns a spacial embedding.

\subsubsection{TF and IDF Metrics Embedding}

All of our enhanced models compute Term Frequency (TF) and Inverse Document Frequency (IDF) metrics for the dataset\cite{nallapati2016abstractive}.
These metrics measure how frequently a term occurs in a document, and how important this term is respectively.
\begin{align}
	\text{TF}(t) &= \frac{\text{Number of times term $t$ appears in a document}}{\text{Total number of terms in the document}} \\[1ex]
	\text{IDF}(t) &= \log \left( \frac{\text{Total number of documents}}{\text{Number of documents with term $t$ in it}} \right)
\end{align}

The inclusion of these embeddings aimed to provide the model with additional information about the importance of words in the context of the entire dataset, potentially improving the quality of the generated summaries.
This is in addition to \emph{positional encodings}, which are added later to the embeddings in transformer models.

\subsection{Loss Function(s)}

\subsubsection{Categorical Cross-Entropy Loss}

Part of our loss function is the Categorical Cross-Entropy Loss\cite{cross_entropy_loss}, which achieves a smooth gradient function by calculating the loss with the logits of the result rather than the result itself.
\begin{equation}
	\mathcal{L}_\text{CCE} = - \frac{1}{N} \frac{1}{W} \sum_{n=1}^{N} \sum^W_{w = 1} \log(p_{n,w,y_{n,w}})
\end{equation}

This loss is calculated separately between each pair of words between the prediction and the target set, where it's averaged between all words and all batches.
While it does predict how close the prediction is to exact value of the target, it does not separate between a slightly different and a completely unrelated word.

\subsubsection{Cosine Similarity Loss}

We can use the embeddings to estimate how close each predicted word to the real target\cite{cosineSimilarity}.
\begin{equation}
	\mathcal{L}_\text{CS} = 1 - \frac{1}{N} \sum_{n=1}^{N} \sum^W_{w = 1} \frac{\mathbf{x}_{n,w} \cdot \mathbf{y}_{n,w}}{\|\mathbf{x}_{n,w}\| \|\mathbf{y}_{n, w}\|}	
\end{equation}
\end{equation}

This loss function would help us reach a result that's meaningful rather than identical to the target, as in the CCE loss.
However, since we are training the embeddings along with the rest of the models, we cannot use it alone as that would converge into embeddings always predicting the same space.

\subsection{Combined Cross Similarity}
We introduce \emph{Combined Cross Similarity Loss}, which combines both CCE and CS losses into a single category in a way that's controlled by a parameter $\varkappa$.
\begin{equation}
    \mathcal{L} = (1 - \varkappa) \mathcal{L}_{\text{CCE}} + \varkappa \mathcal{L}_{\text{CS}}
\end{equation}

A higher $\varkappa$ gives more weight to the cosine-similarity loss component of the loss, while a lower one gives more weight to the categorical cross-entropy component.
The ideal value, along with other components, will be found in a parameter sweep.

Unfortunately, the section in \cref{param_sweep_section} showed that having $\varkappa = 0$ produces a better result for low learning rate.
This probably speaks of the strength of categorical cross-entropy as a loss.
