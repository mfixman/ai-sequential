{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54242090-157b-4a3f-a1a6-0ff11f80747b",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff50e52-abb9-473d-afd0-7d3d4e92cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import nltk.lm as lm\n",
    "from nltk import ngrams\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368fd1dc-46a5-4c3f-ac42-e76506d608a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x753b419a0ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe7b421-dcdd-49c8-a6ee-5daf65b0e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = '<SOS>'\n",
    "eos = '<EOS>'\n",
    "pad = '<PAD>'\n",
    "unk = '<UNK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c90ec-bb24-4422-9a8d-977a18439d20",
   "metadata": {},
   "source": [
    "# Later Things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae8875-5dc9-430b-a138-72730e273ecb",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b70fa9-ca7f-419d-9e3f-aee292139106",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0041b0-3a06-4031-b9b8-8c8ca2587a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pickle.load(open('train_text.pickle', 'rb'))\n",
    "train_high = pickle.load(open('train_high.pickle', 'rb'))\n",
    "vocab = {k: e for e, k, in enumerate(pickle.load(open('vocab.pickle', 'rb')))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a3d001-f8bb-4631-aecb-a67d4b3a2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = [torch.tensor([vocab[x] for x in y]) for y in train_text]\n",
    "tn = pad_sequence(tn, batch_first = True, padding_value = vocab[pad])\n",
    "\n",
    "th = [torch.tensor([vocab[x] for x in y]) for y in train_high]\n",
    "th = pad_sequence(th, batch_first = True, padding_value = vocab[pad])\n",
    "\n",
    "loader = DataLoader(list(zip(tn, th)), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de1c4f-db0c-437f-9297-272ffcb108d5",
   "metadata": {},
   "source": [
    "## Proper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bace1a-0ab0-4c6b-928e-6e5e293f56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = len(vocab),\n",
    "            embedding_dim = 256,\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = 256,\n",
    "            hidden_size = 128,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135c9ea9-d03b-4c59-9aec-4b8541adeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(vocab), embedding_dim = 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size = 256, hidden_size = 128, batch_first = True)\n",
    "        self.out = nn.Linear(128, 128)\n",
    "        self.out2 = nn.Linear(128, len(vocab))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input)\n",
    "        x, h = self.gru(x, hidden)\n",
    "        x = self.out(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out2(x)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6710fa00-143c-4dd9-93fa-089ef44b3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab).to(device)\n",
    "decoder = Decoder(vocab).to(device)\n",
    "optimiser = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr = 1e-3,\n",
    "    weight_decay = 1e-3,\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b6bdb6-a491-4e94-9ecd-2b3971a64d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    for text, high in loader:\n",
    "        text = text.to(device)\n",
    "        high = high.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(text)\n",
    "\n",
    "        assert all(high[:, 0] == vocab[sos])\n",
    "        decoder_input = high[:, 0]\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        loss = tensor(0.).to(device)\n",
    "        text_len = text.size(1)\n",
    "        high_len = high.size(1)\n",
    "        for t in range(1, high_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input.unsqueeze(1), decoder_hidden)\n",
    "            loss += criterion(decoder_output.squeeze(1), high[:, t])\n",
    "            decoder_input = high[:, t]\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec2ac1a-bf2c-44db-9bbb-4bf1597c3d67",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 214.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m high_len \u001b[38;5;241m=\u001b[39m high\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, high_len):\n\u001b[0;32m---> 18\u001b[0m     decoder_output, decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(decoder_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), high[:, t])\n\u001b[1;32m     20\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m high[:, t]\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, h\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 214.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for e in range(epochs):\n",
    "    run_epoch()\n",
    "    print(f'Epoch {e}: loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0d73e-f7a4-4acd-82ae-e7f45c07b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, high in loader:\n",
    "    print(high.size(1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af7dc6-f453-4325-8210-ec394c5ce2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder, decoder, text, high, encoder_output, encoder_hidden, decoder_input, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0729d49-9d2e-4898-9da1-9c406cc3e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd53ef4-7dca-4446-8af5-c5dcf9ec5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfc4e8-6323-40ef-bd8a-3e192fbc2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
